{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket Return to Hubspot ETL\n",
    "\n",
    "This code describes the process to transfer ticket transcation data from Quickbase to Hubspot. This data supports ROI tracking on marketing assets and customer segmentation for sales/marketing strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Extract data from Quickbase\n",
    "\n",
    "Ticketing data needs to be exported from the \"Tickets\" table in Quickbase.\n",
    "\n",
    "When done as part of a daily ETL process, data can be downloaded by exporting this table in Quickbase: <a href=\"https://alexanderbrown.quickbase.com/nav/app/btkpb7gci/table/btkpff4h3/action/q?qid=1000018&skip=0\">Tickets default report</a> <br>\n",
    "Customer data will also need to be exported_dealsed with this table in Quickbase, saved as 'customers.csv', this will be used in Step 3: <a href=\"https://alexanderbrown.quickbase.com/nav/app/btkpb7gci/table/btkpgag95/action/q?qid=1000005&skip=0\">Customers default report</a>\n",
    "\n",
    "However, Quickbase has a limitation in how much data can be exported_dealsed at once. For large exported_dealss representing 30+ days, we need to exported_deals the data in batches as seperate files. To extract all 2024 data, we filter the <link> table on two dimensions to get smaller datasets.\n",
    "* Bowie tickets associated with a customer email address\n",
    "* Bowie tickets not associated with a customer email address\n",
    "* Frederick tickets associated with a customer email address\n",
    "* Frederick tickets not associated with a customer email address\n",
    "\n",
    "\n",
    "The downloaded files should be stored in the same local directory as this script.\n",
    "\n",
    "Columns are then mapped from the quickbase columns to the to HubSpot naming convetions\n",
    "\n",
    "<b> Lastly, we add the appropriate season to the property. </b> When doing future updates, it is key to modify this field to be attributable of the current season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Email</th>\n",
       "      <th>Business unit</th>\n",
       "      <th>Event date</th>\n",
       "      <th>IsScanned</th>\n",
       "      <th>Section</th>\n",
       "      <th>Ticket Type</th>\n",
       "      <th>TicketPrice</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>Close Date</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220577.0</td>\n",
       "      <td>redgoing@hotmail.com</td>\n",
       "      <td>Baysox</td>\n",
       "      <td>08-09-2024</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>Group Ticket</td>\n",
       "      <td>$18</td>\n",
       "      <td>478864</td>\n",
       "      <td>08-02-2024 12:51 PM</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220577.0</td>\n",
       "      <td>redgoing@hotmail.com</td>\n",
       "      <td>Baysox</td>\n",
       "      <td>08-09-2024</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>Group Ticket</td>\n",
       "      <td>$18</td>\n",
       "      <td>478864</td>\n",
       "      <td>08-02-2024 12:51 PM</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220577.0</td>\n",
       "      <td>redgoing@hotmail.com</td>\n",
       "      <td>Baysox</td>\n",
       "      <td>08-09-2024</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>Group Ticket</td>\n",
       "      <td>$18</td>\n",
       "      <td>478864</td>\n",
       "      <td>08-02-2024 12:51 PM</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220577.0</td>\n",
       "      <td>redgoing@hotmail.com</td>\n",
       "      <td>Baysox</td>\n",
       "      <td>08-09-2024</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>Group Ticket</td>\n",
       "      <td>$18</td>\n",
       "      <td>478864</td>\n",
       "      <td>08-02-2024 12:51 PM</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220577.0</td>\n",
       "      <td>redgoing@hotmail.com</td>\n",
       "      <td>Baysox</td>\n",
       "      <td>08-09-2024</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>Group Ticket</td>\n",
       "      <td>$18</td>\n",
       "      <td>478864</td>\n",
       "      <td>08-02-2024 12:51 PM</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer ID                 Email Business unit  Event date IsScanned  \\\n",
       "0    1220577.0  redgoing@hotmail.com        Baysox  08-09-2024        no   \n",
       "1    1220577.0  redgoing@hotmail.com        Baysox  08-09-2024        no   \n",
       "2    1220577.0  redgoing@hotmail.com        Baysox  08-09-2024        no   \n",
       "3    1220577.0  redgoing@hotmail.com        Baysox  08-09-2024        no   \n",
       "4    1220577.0  redgoing@hotmail.com        Baysox  08-09-2024        no   \n",
       "\n",
       "  Section   Ticket Type TicketPrice  TransactionID           Close Date  \\\n",
       "0     104  Group Ticket         $18         478864  08-02-2024 12:51 PM   \n",
       "1     104  Group Ticket         $18         478864  08-02-2024 12:51 PM   \n",
       "2     104  Group Ticket         $18         478864  08-02-2024 12:51 PM   \n",
       "3     104  Group Ticket         $18         478864  08-02-2024 12:51 PM   \n",
       "4     104  Group Ticket         $18         478864  08-02-2024 12:51 PM   \n",
       "\n",
       "   Season  \n",
       "0    2024  \n",
       "1    2024  \n",
       "2    2024  \n",
       "3    2024  \n",
       "4    2024  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read each CSV into a separate dataframe\n",
    "bowie_email = pd.read_csv(\"Baysox 2024 Email.csv\")\n",
    "bowie_no_email = pd.read_csv(\"Baysox 2024 no Email.csv\")\n",
    "keys_email = pd.read_csv(\"Keys 2024 Email.csv\")\n",
    "keys_no_email = pd.read_csv(\"Keys 2024 no Email.csv\")\n",
    "\n",
    "# Concatenate all dataframes into a single one\n",
    "tickets = pd.concat([bowie_email, bowie_no_email, keys_email, keys_no_email], ignore_index=True)\n",
    "\n",
    "# Dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    \"Customer ID\": \"Customer ID\",\n",
    "    \"Customer ID - Email\": \"Email\",\n",
    "    \"Event ID - Sponsor - Team\": \"Business unit\",\n",
    "    \"Event ID - Event Date\": \"Event date\",\n",
    "    \"IsScanned\": \"IsScanned\",\n",
    "    \"Section\": \"Section\",\n",
    "    \"Ticket product - Ticket Type\": \"Ticket Type\",\n",
    "    \"TicketPrice\": \"TicketPrice\",\n",
    "    \"TransactionID\": \"TransactionID\",\n",
    "    \"Transaction Date\": \"Close Date\",\n",
    "    \"Event ID - Sponsor - Year\": \"Season\"\n",
    "}\n",
    "\n",
    "# Rename the columns in the dataframe\n",
    "tickets = tickets.rename(columns=column_mapping)\n",
    "\n",
    "# Add Season column used to distinguish the season these tickets are from\n",
    "Season = 2024\n",
    "\n",
    "tickets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Deal Data Transformation\n",
    "\n",
    "We perform several data transformations to improve data quality and make the data acceptable for import to Hubspot\n",
    "\n",
    "1. <b> Email domain corrections </b> -- Replace invalid domain exentions like \".comm\" with \".com\" or their likely equaivalent\n",
    "2. <b> Define a new transcation id </b> -- Ticket Return's [Transaction ID] values are only unique to the organization (Bowie or Frederick). We set [Transaction ID] = \"[Organization]-[Transaction ID]\"\n",
    "3. <b> Convert tickets scanned to 1,0 </b> -- Reconfigure [IsScanned] to binary to sum when grouping to get the total number of tickets scanned per transaction\n",
    "4. <b> Set ticket price to numeric </b> -- Originally had issues with some prices being in different formats, this ensures there are no issues when calculating total price\n",
    "5. <b> Fill null types to Unknown </b> -- Mark tickets without a type as unknown to distinguish them\n",
    "6. <b> Set price for certain ticket types to 0 </b> -- Certain deals are already tracked in HubSpot, we set these types to zero as to not double count our revenues\n",
    "7. <b> Adjusting close date </b> -- Some transactions have tickets without close dates, these are populated with the same date as the rest of the transaction or the event date as when importing to HubSpot if there is no closed date on a closed won deal it will default to the create date and we don't want deals to have close dates after the events for individual tickets\n",
    "\n",
    "Following the transformations we store our dataframe in a new one called line_items to use later to import the line items connecting to the deals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaxStevens\\AppData\\Local\\Temp\\ipykernel_16836\\4285153792.py:69: UserWarning: NULL values detected in 'Close Date'. This should not occur as per Quickbase requirements. Please check upstream data feed from TR to Quickbase.\n",
      "  warnings.warn(\"NULL values detected in 'Close Date'. This should not occur as per Quickbase requirements. Please check upstream data feed from TR to Quickbase.\")\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Function to filter out malformed emails (doesn't contain @ symbol)\n",
    "def format_emails(email):\n",
    "    if pd.notna(email) and '@' in email:\n",
    "        return email\n",
    "    return ''\n",
    "\n",
    "# Function to fix invalid emails\n",
    "def format_invalid_emails(email):\n",
    "    com_variations = [',com', '.cim', '.ocm', '.C0M', '.comm', '. COM', '.coom', '.cop', '.oom', '.lcom', '.con', '.co,m', '..com', '.cpm', '.cpom', '.col', '.co,', '.xom', '.fom', '.coke', 'cok', '.come', '..com']\n",
    "    net_variations = [',net', '.met', '.ent', '.nat', '.nt', '.ner', '.ney', '.lnet', '. net', '.nety', '.neto', '.ncet', '/net', '.nedt', '.nett', '.nbet']\n",
    "    \n",
    "    # Skip processing if the email is NaN or empty\n",
    "    if pd.isna(email) or email == '':\n",
    "        return email\n",
    "    \n",
    "    # Remove spaces from the email\n",
    "    email = email.replace(\" \", \"\")\n",
    "    \n",
    "    # Fix .com variations\n",
    "    for variation in com_variations:\n",
    "        if variation in email:\n",
    "            email = email.replace(variation, \".com\")\n",
    "            break\n",
    "    \n",
    "    # Fix .net variations\n",
    "    for variation in net_variations:\n",
    "        if variation in email:\n",
    "            email = email.replace(variation, \".net\")\n",
    "            break\n",
    "\n",
    "    # Remove consecutive dots\n",
    "    while '..' in email:\n",
    "        email = email.replace('..', '.')\n",
    "    \n",
    "    return email\n",
    "\n",
    "# Apply the functions, skipping NaN values\n",
    "tickets['Email'] = tickets['Email'].apply(lambda x: format_emails(x) if pd.notna(x) else x)\n",
    "tickets['Email'] = tickets['Email'].apply(lambda x: format_invalid_emails(x) if pd.notna(x) else x)\n",
    "\n",
    "#tickets.to_csv('test.csv')\n",
    "tickets['TransactionID'] = tickets['Business unit'] + '-' + tickets['TransactionID'].astype(str)\n",
    "\n",
    "# 1. Change 'IsScanned' from 'yes/no' to 1/0\n",
    "tickets['IsScanned'] = tickets['IsScanned'].apply(lambda x: 1 if x.lower() == 'yes' else 0)\n",
    "\n",
    "# Remove any dollar signs and commas from 'TicketPrice' and convert to float\n",
    "tickets['TicketPrice'] = tickets['TicketPrice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Replace NaN values in 'Ticket Type' with 'Unknown'\n",
    "tickets['Ticket Type'] = tickets['Ticket Type'].fillna('Unknown')\n",
    "\n",
    "# Define the ticket types that should have PriceCorrected set to 0\n",
    "invalid_ticket_types = ['Group Ticket', 'Ticket Plan', 'Ticket Package', 'Unknown', 'Non-Ticket Inventory']\n",
    "\n",
    "# Create a new column 'PriceCorrected' based on the condition\n",
    "tickets['PriceCorrected'] = tickets.apply(lambda row: 0 if row['Ticket Type'] in invalid_ticket_types else row['TicketPrice'], axis=1)\n",
    "\n",
    "# Convert 'Close Date' to datetime with time\n",
    "tickets['Close Date'] = pd.to_datetime(tickets['Close Date'], errors='coerce')\n",
    "\n",
    "# Convert 'Event date' to datetime (without time)\n",
    "tickets['Event date'] = pd.to_datetime(tickets['Event date'], errors='coerce').dt.date\n",
    "\n",
    "# Check for NULL values in 'Close Date' and issue a warning if any are found\n",
    "if tickets['Close Date'].isna().any():\n",
    "    warnings.warn(\"NULL values detected in 'Close Date'. This should not occur as per Quickbase requirements. Please check upstream data feed from TR to Quickbase.\")\n",
    "\n",
    "# Continue with storing missing 'Close Date' rows for reference\n",
    "missing_closed_dates = tickets[tickets['Close Date'].isna()].copy()\n",
    "missing_closed_dates.drop_duplicates(subset=['TransactionID']).to_csv('missing_closed_dates.csv')\n",
    "\n",
    "\n",
    "# Fill missing 'Close Date' values using 'Event date' with a default time of 9 AM\n",
    "tickets['Close Date'] = tickets['Close Date'].fillna(\n",
    "    pd.to_datetime(tickets['Event date'].astype(str) + ' 09:00')\n",
    ")\n",
    "\n",
    "#Save line items for later use after we group by for transactions\n",
    "line_items = tickets\n",
    "\n",
    "tickets.to_csv('line items.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Import New Contacts to Hubspot\n",
    "\n",
    "We must first migrate our unique contacts to HubSpot to ensure we don't have errors upon importing deals, there are two types of errors we must account for:\n",
    "\n",
    "1. <b> Invalid Email </b> -- This we solve above when correcting the email domains, HubSpot will only import valid email addresses\n",
    "2. <b> Invalid Duplicate ID </b> -- This error occurs when a contact is created in an import then appears again in the import. Since we are importing multiple deals associating to contacts we are at risk of this error. This only occurs when a contact is created in the import so by first importing the contacts then the deals we ensure that we won't create any contacts when importing deals removing this risk\n",
    "\n",
    "When importing these contacts we want to ensure we are including all the data exported from QuickBase into HubSpot. We also want to ensure this data is clean and that contacts are being put in the correct business unit. The exported file from Step 1 has the correct columns needed. This code below will clean and match them to get us an export of unique emails with cleaned information\n",
    "\n",
    "1. <b> Find Business units </b> -- Our customers are exported with the # of tickets they purchased for each team. We use this to find if they belong in a unique business unit or both\n",
    "2. <b> Add Contact owner </b> -- Based on the business units the respective contact owners are added\n",
    "3. <b> Clean customer emails </b> -- Since we are matching on emails to those pulled from our tickets, we must ensure they are in the same format as them. To do this we run them through the same email cleaning process then set both emails to be lowercase in order to help with matching and uniqueness \n",
    "4. <b> Clean customer columns </b> -- We then want to clean the customers data that we will be importing into HubSpot. This includes formating name, address, and phone number data as well as removing the columns no longer needed\n",
    "5. <b> Merge and drop duplicates </b> -- Last step is to merge and drop the duplicates so we don't import the same email twice which will cause an error\n",
    "\n",
    "After these steps are followed you are left with a file to import into HubSpot which will ensure all of our customers contacts are in the system before importing the deals which will allow them to match. When importing, match each column to its corresponding HubSpot column, all of these are self explanatory when matching. It is important that you <b> do not overwrite </b> is selected for all columns\n",
    "\n",
    "##### <b> Do not overwrite columns on contact import </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 3. Remove NaN and empty emails, ensure uniqueness\n",
    "unique_emails = tickets['Email'].dropna().replace('', pd.NA).dropna().str.strip().unique()\n",
    "\n",
    "# Create a new DataFrame with unique emails\n",
    "tickets_unique_emails = pd.DataFrame(unique_emails, columns=['Email'])\n",
    "\n",
    "# Import the customers into a DataFrame\n",
    "customers = pd.read_csv('customers.csv')\n",
    "\n",
    "# Function to format phone numbers\n",
    "def format_phone_number(phone):\n",
    "    if pd.isna(phone) or isinstance(phone, str) and not re.search(r'\\d', phone):\n",
    "        return None\n",
    "    else:\n",
    "        clean_phone = re.sub(r'[^\\d]', '', str(phone))  # Remove all non-digit characters\n",
    "        if re.match(r'(\\d{10})$', clean_phone):\n",
    "            # category 1, complete number\n",
    "            formatted_number = f'+1{clean_phone}'\n",
    "            return formatted_number\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to format zip codes\n",
    "def format_zip_codes(zip_code):\n",
    "    if pd.isna(zip_code) or not isinstance(zip_code, str):\n",
    "        return ''\n",
    "    zip_code = re.sub(r'\\D', '', zip_code)  # Remove non-digit characters\n",
    "    if len(zip_code) >= 5:\n",
    "        return zip_code[:5]  # Return the first 5 characters\n",
    "    return ''\n",
    "\n",
    "# Function to determine the business unit\n",
    "def determine_business_unit(row):\n",
    "    bowie_tickets = int(row['# of Bowie Tickets'])\n",
    "    frederick_tickets = int(row['# of Frederick Tickets'])\n",
    "    business_units = []\n",
    "    \n",
    "    if bowie_tickets > 0:\n",
    "        business_units.append('Bowie Baysox')\n",
    "    if frederick_tickets > 0:\n",
    "        business_units.append('Frederick Keys')\n",
    "    \n",
    "    return ';'.join([''] + business_units) if business_units else None\n",
    "\n",
    "# Function to determine the contact owner for Bowie Baysox\n",
    "def contact_owner_bowie(business_unit):\n",
    "    if business_unit and 'Bowie Baysox' in business_unit:\n",
    "        return 'info@baysox.com'\n",
    "    return None\n",
    "\n",
    "# Function to determine the contact owner for Frederick Keys\n",
    "def contact_owner_frederick(business_unit):\n",
    "    if business_unit and 'Frederick Keys' in business_unit:\n",
    "        return 'info@frederickkeys.com'\n",
    "    return None\n",
    "\n",
    "# Create the 'Business Unit' column\n",
    "customers['Business Unit'] = customers.apply(determine_business_unit, axis=1)\n",
    "\n",
    "# Create the 'Contact Owner - Bowie Baysox' column\n",
    "customers['Contact Owner - Bowie Baysox'] = customers['Business Unit'].apply(contact_owner_bowie)\n",
    "\n",
    "# Create the 'Contact Owner - Frederick Keys' column\n",
    "customers['Contact Owner - Frederick Keys'] = customers['Business Unit'].apply(contact_owner_frederick)\n",
    "\n",
    "# Apply the functions to clean 'Email', skipping NaN values\n",
    "customers['Email'] = customers['Email'].apply(lambda x: format_emails(x) if pd.notna(x) else x)\n",
    "customers['Email'] = customers['Email'].apply(lambda x: format_invalid_emails(x) if pd.notna(x) else x)\n",
    "\n",
    "# Convert email addresses to lowercase in both dataframes to allow for consistent matching and uniqueness\n",
    "customers['Email'] = customers['Email'].str.lower()\n",
    "tickets_unique_emails['Email'] = tickets_unique_emails['Email'].str.lower()\n",
    "\n",
    "# Set country to US (comes as USA)\n",
    "customers['Country'] = 'US'\n",
    "\n",
    "# Ensure columns are in title case\n",
    "customers['First Name'] = customers['First Name'].str.title()\n",
    "customers['Last Name'] = customers['Last Name'].str.title()\n",
    "customers['Addr1'] = customers['Addr1'].str.title()\n",
    "customers['Addr2'] = customers['Addr2'].str.title()\n",
    "customers['City'] = customers['City'].str.title()\n",
    "\n",
    "# Apply the function to the 'Phone' column to clean\n",
    "customers['Phone'] = customers['Phone'].apply(format_phone_number)\n",
    "\n",
    "# Apply the function to the 'Zip' column to clean\n",
    "customers['Zip'] = customers['Zip'].apply(format_zip_codes)\n",
    "\n",
    "# Drop specified columns\n",
    "customers = customers.drop(columns=['# of Bowie Tickets', '# of Frederick Tickets'])\n",
    "\n",
    "# Rename columns in the customers dataframe\n",
    "customers.rename(columns={\n",
    "    'Addr1': 'Street Address',\n",
    "    'Addr2': 'Street Address Line 2',\n",
    "    'Zip': 'Postal Code'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge on 'Email' column\n",
    "cleaned_emails = pd.merge(customers, tickets_unique_emails, on='Email', how='inner')\n",
    "\n",
    "# Drop duplicate emails if any exist after merging\n",
    "cleaned_emails = cleaned_emails.drop_duplicates(subset='Email')\n",
    "\n",
    "# Save file for Import\n",
    "cleaned_emails.to_csv('2024.11.11 - Attain - Individual Tickets Customer Update.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Transform Tickets into Transcations for Import\n",
    "\n",
    "Now we have all our tickets cleaned up we are ready to group these into their specific transactions which will be the deals we will import\n",
    "\n",
    "1. <b> Sort by Email </b> -- For some tickets in QuickBase the email did not populate all instances, this ensure we will have an email value if there is one associated with any of the tickets in the transaction\n",
    "2. <b> Define prioritization of tickets </b> -- We want to prioritize what ticket type will assume the primary name of the [TicketType] in the Deal. When we import our Deals into HubSpot we can only have one TicketType and since we are counting revenue based on Individual, Exchange, and Comp only those will be our priority. When we import the line items it will include the further breakdown into each individual type\n",
    "3. <b> Group into transactions </b> -- Grouping is done by [TransactionID] and [Event Date] so we can see the specific breakdown of games attended. After grouping the required HubSpot properties are added and we set the [Deal Owner] and [BusinessUnit] to the correct name. These Deals are then named and ready for import into HubSpot.\n",
    "\n",
    "We then split the DataFrame into two, one with known email and one with unknown email <br>\n",
    "\n",
    "The reason for doing this is when importing into HubSpot we want to do two different imports as importing some with emails and some without will cause errors. The way to do each of these imports is listed below:\n",
    "1. <b> Email</b>: Select Deals and Contacts on import, do create deals only and update contacts only\n",
    "2. <b> No Email</b>: Select Deals on import, do create deals only\n",
    "\n",
    "All column names match those they are importing to but one area to keep a note on is when importing the [Close Date] property will default to be a contact property. <b> Close Date must be switched to the deal property under the same name or the deal closed dates will be set to the import date. </b>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets.sort_values(by=['Email', 'TransactionID'], ascending=[True, True])\n",
    "\n",
    "# Define a function to prioritize TicketType\n",
    "def prioritize_ticket_type(ticket_types):\n",
    "    # Define the priority order\n",
    "    priority_order = ['Individual Ticket', 'Exchange Ticket', 'Comp Ticket', 'Ticket Plan', 'Group Ticket', 'Ticket Package']\n",
    "    \n",
    "    # Remove NaN values and sort based on the priority\n",
    "    ticket_types = [t for t in ticket_types if pd.notna(t)]\n",
    "    for priority in priority_order:\n",
    "        if priority in ticket_types:\n",
    "            return priority\n",
    "    return 'Other'  # Default if none of the types match\n",
    "\n",
    "# Group by TransactionID and Event Date\n",
    "deals = tickets.groupby(['TransactionID', 'Event date']).agg(\n",
    "    TotalTickets=('TransactionID', 'size'),            # Count the number of rows\n",
    "    TotalPrice=('PriceCorrected', 'sum'),              # Sum of PriceCorrected\n",
    "    TicketType=('Ticket Type', prioritize_ticket_type),               # First instance of Ticket Type\n",
    "    Section=('Section', 'first'),                      # First instance of Section\n",
    "    BusinessUnit=('Business unit', 'first'),           # First instance of Business Unit\n",
    "    NumberOfTicketsScanned=('IsScanned', 'sum'),       # Sum of IsScanned\n",
    "    CloseDate=('Close Date', 'first'),                 # First instance of Close Date\n",
    "    Email=('Email', 'first')                           # First instance of Email\n",
    ").reset_index()\n",
    "\n",
    "# Add the new columns\n",
    "deals['Pipeline'] = 'Individual Tickets Pipeline'            # Set Pipeline as 'Individual Tickets'\n",
    "deals['Deal Stage'] = 'Closed Won'                  # Set Deal Stage as 'Closed Won'\n",
    "deals['Deal type'] = 'Individual Ticket'            # Set Deal type as 'Individual Ticket\n",
    "deals['Season'] = Season                            # Set Season as the current season\n",
    "\n",
    "# Assign Deal Owner based on Business Unit\n",
    "deals['Deal Owner'] = deals['BusinessUnit'].map({\n",
    "    'Baysox': 'info@baysox.com',\n",
    "    'Keys': 'info@frederickkeys.com'\n",
    "}).fillna('')\n",
    "\n",
    "# Convert 'Event date' to string before concatenation\n",
    "deals['Deal Name'] = deals['BusinessUnit'] + ' ' + deals['Event date'].astype(str) + ' ' + deals['TotalTickets'].astype(str) + ' Tickets'\n",
    "\n",
    "# Map the BusinessUnit values and assign them back to the column\n",
    "deals['BusinessUnit'] = deals['BusinessUnit'].map({\n",
    "    'Baysox': 'Bowie Baysox',\n",
    "    'Keys': 'Frederick Keys'\n",
    "}).fillna('')\n",
    "\n",
    "# Rename columns in the Deals dataframe\n",
    "deals.rename(columns={\n",
    "    'TransactionID': 'TicketReturn transaction number',\n",
    "    'TotalTickets': 'Number of Tickets',\n",
    "    'TotalPrice': 'Amount',\n",
    "    'TicketType': 'Individual Ticket Type'\n",
    "}, inplace=True)\n",
    "\n",
    "# Separate rows where Email is known and where it is not\n",
    "deals_known_email = deals[deals['Email'].notna()]\n",
    "deals_unknown_email = deals[deals['Email'].isna()]\n",
    "\n",
    "# Drop specified columns\n",
    "deals_unknown_email = deals_unknown_email.drop(columns=['Email'])\n",
    "\n",
    "# exported_deals each to a separate CSV file\n",
    "deals_known_email.to_csv('2024.11.11 - Attain - Individual Baseball Tickets Email.csv', index=False)\n",
    "deals_unknown_email.to_csv('2024.11.11 - Attain - Individual Baseball Tickets No Email.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Export Deals from HubSpot and Merge with Line Items\n",
    "\n",
    "When updating deals in HubSpot you need the Record ID of the deal. Since line items will be added onto the deals we need to <b> export the deals we created along with their Record ID, TicketReturn transaction number, and Event Date </b> to merge the Record IDs for the Deals to the line items. \n",
    "\n",
    "In doing this we are using the <b> line_items </b> dataframe that we had saved previously after cleaning our data <br>\n",
    "Export the deals from HubSpot and save them as <b> export.csv </b> in the same location as the code\n",
    "\n",
    "1. <b> Match the Event Date columns </b> -- Since we are merging on [Event Date] we need to ensure that the formats of the event dates are the same across dataframes\n",
    "2. <b> Merge data </b> -- Here we merge the data to get the Record ID of deals associated with the line items. We do a left join because we know that for each deal there must be associating line items as deals all have at least one ticket associated with them and the line items are representative of the tickets. However, it is import to <b> check all line items have a matching deal </b> before importing, therefore a warning is sent if that is not the case. This could be due to an incorrect export and should be checked.\n",
    "3. <b> Create Line Item properties </b> -- There are 3 required properties for creating a line item in HubSpot. These 3 properties are Name, Quantity, and Price. To create these we start with a naming convention which is the 'Ticket Type + Section'. We follow this up with creating the Price and Quantity properties. These take all the tickets with the same name from the transaction and counts the number of tickets and takes the average price in order to have the amount in our line items match the amount of the deal.\n",
    "\n",
    "Once these are merged and added we can export this file for import into HubSpot. HubSpot can't take line items with negative values so it is important to set line items with values less than zero to equal zero before importing (there should be none anyway)\n",
    "\n",
    "To import into HubSpot:\n",
    "1. Select Deals and Line Items\n",
    "2. Set to update Deals and create Line Items\n",
    "3. Select Record ID as a deal property matching to Record ID\n",
    "4. Set Name, Quantity, and Price to be line item properties under the same names\n",
    "5. This should result in no errors in the preview and can go ahead and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaxStevens\\AppData\\Local\\Temp\\ipykernel_16836\\2564772614.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  line_items = pd.read_csv('line items.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lines items have been merged to a deal\n"
     ]
    }
   ],
   "source": [
    "exported_deals = pd.read_csv('export.csv')\n",
    "line_items = pd.read_csv('line items.csv')\n",
    "\n",
    "# Merge on 'Event Date' and 'TicketReturn transaction number' from exported_deals\n",
    "merged_line_items = pd.merge(line_items, exported_deals[['Event Date', 'TicketReturn transaction number', 'Record ID']],\n",
    "                     left_on=['Event date', 'TransactionID'], \n",
    "                     right_on=['Event Date', 'TicketReturn transaction number'],\n",
    "                     how='left')  # Use 'left' join to keep all rows from tickets\n",
    "\n",
    "# Check for any unmatched line items (where Record ID is NaN)\n",
    "unmatched_line_items = merged_line_items[merged_line_items['Record ID'].isna()]\n",
    "\n",
    "# Issue a warning if any unmatched line items are found\n",
    "if not unmatched_line_items.empty:\n",
    "    warnings.warn(f\"{len(unmatched_line_items)} line items have no matching deal record. Find an associated deal before importing, check HubSpot on transaction number to ensure deal was originally imported.\")\n",
    "else:\n",
    "    print('All lines items have been merged to a deal')\n",
    "\n",
    "# Drop the 'TicketReturn transaction number' column as it's no longer needed after the merge\n",
    "merged_line_items = merged_line_items.drop('TicketReturn transaction number', axis=1)\n",
    "\n",
    "merged_line_items['Line Item Name'] = merged_line_items['Ticket Type'] + ' - ' + line_items['Section']\n",
    "\n",
    "# Group by Record ID and Line Item Name\n",
    "line_items_cleaned = merged_line_items.groupby(['Record ID', 'Line Item Name']).agg(\n",
    "    Price=('PriceCorrected', 'mean'),  # Calculate the average of PriceCorrected as Price\n",
    "    Quantity=('Line Item Name', 'count')  # Count the number of rows as Quantity\n",
    ").reset_index()\n",
    "\n",
    "# Set any negative Price values to 0 and format to two decimal places\n",
    "line_items_cleaned['Price'] = line_items_cleaned['Price'].apply(lambda x: max(float(x), 0)).round(2)\n",
    "\n",
    "line_items_cleaned.to_csv('2024.11.11 - Attain - Individual Baseball Line Items.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
